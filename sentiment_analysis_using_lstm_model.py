# -*- coding: utf-8 -*-
"""Sentiment_Analysis_using_LSTM_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Praveen76/Sentiment-Analysis-using-LSTM-Model/blob/main/Sentiment_Analysis_using_LSTM_Model.ipynb

## Learning Objectives

At the end of the experiment, you will be able to:

* explore the text data
* understand and perform text pre-processing
* build an LSTM model
* train the model on the IMDB reviews dataset

## Sentiment Classification

Sentiment classification is the automated process of identifying and classifying emotions in the text as positive sentiment, negative sentiment, or neutral sentiment based on the opinions expressed within. It helps determine the nature and extent of feelings conveyed using Natural Language Processing (NLP) to understand what customers say or feel about your brand, products, and services.

Manually reading, classifying, and sorting text data can take a long time. You can lose the essence of what's important when you get stuck manually analyzing and classifying positive and negative sentiments.

Sentiment classification has seen tremendous growth in recent years. Many businesses use it to better understand their customers' feelings, behavior, preferences, and needs.

Automated classification methods enable you to analyze reviews, comments, survey responses, and other public opinions faster. They analyze data to determine the overall tone toward your brand and products, as well as the broader industry and market trends and insights.

### Recurrent Neural Network (RNN)

Recurrent neural network is a type of neural network that is proven to work well with sequence data. Since text is a sequence of words, a recurrent neural network is an automatic choice to solve text-related problems. Here, we will use an LSTM (Long Short Term Memory network) which is a variant of RNN, to solve a movie reviews-based sentiment classification problem.

An LSTM unit consists of a cell, an input gate, an output gate, and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.

LSTM networks are well-suited to classification based on time series data and deal well with the exploding and vanishing gradient problems that can be encountered when training traditional RNNs.

<img style="-webkit-user-select: none;margin: auto;" src="https://datascienceimages.s3.eu-north-1.amazonaws.com/Sentiment_Analysis_using_LSTM_Model/RNN.png?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHQaCmV1LW5vcnRoLTEiRzBFAiEAmOZQhT%2BYM40tIrgSCC0juQ3jYb2AAM%2F1Fbz6dD8McLICIHg%2Fvrzjk8lvADAhmWpZrYL9TF3PwvkzKpdWA8hZeRQoKu0CCM3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMOTc1MDUwMDY4NjU4IgyeGfj2LlL0RNsVYPMqwQJM4d9h4CmJwEyTBC7qZEzc29kl0a%2FrmErIOHAztB%2FrvmsLwfZSviafkN0S0Rgnb9ZUlQLlOLgmJAh%2Boo5wqCAD6Vph5HKDP0UmiithH%2BlXOUIiDE51ykTUEA3w1iSSHMyOs7vDBsLp%2FMFpvpCG3V2XnlQ5Iqgo422ee763aDs%2FzuanCRZpfGDaLTo%2BgTWY0vQntjWJILYrZeOw0tpLpzZArag7KROagc0J8JjBvK0M%2Fz02ApRdrd4sspTDVFraDlGzFJekxjfCDEkf0gSQy7gutoZ2hujhd%2BonPc2Sob988wEY1BUTJWathwBoXphrbMZIS6LhUp1rTnDl8w7hv54XZ4ORo9w4oXVsZdG%2FDbp2dUX6yVHFY3mjMGdU6fEQ1j2AlzySCTm4T%2FzJpXst4CisnbG6opNd6Os2b78VkYfQnKAwtdPmsQY6swKV5d%2BjoEU9poWR0HUudhbWdbIOfv1xKG64htIRW3NpNgi0OonJi5zOPTw5aWoAvi5hbnA7akONMJtwKRTgl9t30qtpFA8DwvmNL%2BUt%2FNsayqUy%2BrKPtD%2FtfVYyu82iXyxz4EyCSXPTgcCREW61a0z%2F8HtRH76JayEUAc7PAypjS861xn7SFWDzldlYWE8H2bDvjOQdf82VBKm67cP5CPWaxKpPkpTJFAR82hOQeFordG1wctY1YO5AOZnqv8Wd%2B7Tz9rQYNYk9xUihZZHgrvYzMuQWKcj%2By2eMBR7uWxP5LR48KBnL6edR4ZCv5H5LYCGjIREzJAImRO9qbDptQLHW%2B943UkCxOSqNdCIQjgRoWe84M93lB6rTQjLowcqWYU9YJDY9V73Jfw8Qz3mx6HxtnhdV&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240507T111714Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIA6GBMDKKZK2QSB44A%2F20240507%2Feu-north-1%2Fs3%2Faws4_request&X-Amz-Signature=8c28faec7e9968f0be295c18e41b3a524217ef90c1717c2335c835c566873755" width="500" height="250">


<img style="-webkit-user-select: none;margin: auto;" src="https://datascienceimages.s3.eu-north-1.amazonaws.com/Sentiment_Analysis_using_LSTM_Model/LSTM.png?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHQaCmV1LW5vcnRoLTEiRzBFAiEAmOZQhT%2BYM40tIrgSCC0juQ3jYb2AAM%2F1Fbz6dD8McLICIHg%2Fvrzjk8lvADAhmWpZrYL9TF3PwvkzKpdWA8hZeRQoKu0CCM3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMOTc1MDUwMDY4NjU4IgyeGfj2LlL0RNsVYPMqwQJM4d9h4CmJwEyTBC7qZEzc29kl0a%2FrmErIOHAztB%2FrvmsLwfZSviafkN0S0Rgnb9ZUlQLlOLgmJAh%2Boo5wqCAD6Vph5HKDP0UmiithH%2BlXOUIiDE51ykTUEA3w1iSSHMyOs7vDBsLp%2FMFpvpCG3V2XnlQ5Iqgo422ee763aDs%2FzuanCRZpfGDaLTo%2BgTWY0vQntjWJILYrZeOw0tpLpzZArag7KROagc0J8JjBvK0M%2Fz02ApRdrd4sspTDVFraDlGzFJekxjfCDEkf0gSQy7gutoZ2hujhd%2BonPc2Sob988wEY1BUTJWathwBoXphrbMZIS6LhUp1rTnDl8w7hv54XZ4ORo9w4oXVsZdG%2FDbp2dUX6yVHFY3mjMGdU6fEQ1j2AlzySCTm4T%2FzJpXst4CisnbG6opNd6Os2b78VkYfQnKAwtdPmsQY6swKV5d%2BjoEU9poWR0HUudhbWdbIOfv1xKG64htIRW3NpNgi0OonJi5zOPTw5aWoAvi5hbnA7akONMJtwKRTgl9t30qtpFA8DwvmNL%2BUt%2FNsayqUy%2BrKPtD%2FtfVYyu82iXyxz4EyCSXPTgcCREW61a0z%2F8HtRH76JayEUAc7PAypjS861xn7SFWDzldlYWE8H2bDvjOQdf82VBKm67cP5CPWaxKpPkpTJFAR82hOQeFordG1wctY1YO5AOZnqv8Wd%2B7Tz9rQYNYk9xUihZZHgrvYzMuQWKcj%2By2eMBR7uWxP5LR48KBnL6edR4ZCv5H5LYCGjIREzJAImRO9qbDptQLHW%2B943UkCxOSqNdCIQjgRoWe84M93lB6rTQjLowcqWYU9YJDY9V73Jfw8Qz3mx6HxtnhdV&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240507T111748Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIA6GBMDKKZK2QSB44A%2F20240507%2Feu-north-1%2Fs3%2Faws4_request&X-Amz-Signature=d13f6d30a7e60c77a74525d861423382d794141eb5c92c1f4002895e2c3bf246" width="500" height="250">

## Dataset Description

The dataset used in this experiment is the [IMDB movie review](http://ai.stanford.edu/~amaas/data/sentiment/) dataset. This dataset for binary sentiment classification contains around 50k movie reviews with the following attributes:

* **review:** text based review of each movie
* **sentiment:** positive or negative sentiment value

### Importing Required Libraries
"""

from gettext import install
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import re
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from bs4 import BeautifulSoup          # BeautifulSoup is a useful library for extracting data from HTML and XML documents
from numpy import array
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.layers import Activation, Dropout
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import one_hot, Tokenizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, GlobalMaxPooling1D, Dense, Embedding, LSTM, GRU
import pandas.testing as tm

"""### Load the Dataset"""

import yaml
import json

from google.colab import drive

# Mount Google Drive with force remount
drive.mount('/content/drive', force_remount=True)


# Load relevant API Keys
file_path = '/content/drive/MyDrive/.API_KEYS/API_KEYS.yml'

with open(file_path, 'r') as file:
    api_keys = yaml.safe_load(file)


# Extract Kaggle username and key
aws_access_key_id = api_keys['AWS']['AWS_ACCESS_KEY_ID']
aws_secret_access_key = api_keys['AWS']['AWS_SECRET_ACCESS_KEY']

import os
os.environ['AWS_ACCESS_KEY_ID']=aws_access_key_id
os.environ['AWS_SECRET_ACCESS_KEY']=aws_secret_access_key


import subprocess
subprocess.run(['pip', 'install', 'awscli'])
subprocess.run(['aws', 'configure'])
subprocess.run(['aws', 's3', 'cp', 's3://datasciencedataset/IMDB_Dataset.csv', '.'])


movie_reviews = pd.read_csv("/content/IMDB_Dataset.csv")

"""### Exploratory Data Analysis"""

# Check the shape of the data
movie_reviews.shape

movie_reviews.head()

# Check for null Values
movie_reviews.isnull().sum()

# Let us view one of the reviews
movie_reviews["review"][5]

# View the distribution of positive and negative sentiments in the dataset
sns.countplot(x='sentiment', data = movie_reviews)
plt.show()

"""We can see above that there are an equal number of positive and negative sentiment reviews in the dataset.

###Data Preprocessing

Remove any html tags, punctuations, special characters, stopwords etc. from the review text.

- **Remove HTML, XML, etc.**
"""

# removing the html strips
def strip_html(text):
    # BeautifulSoup is a useful library for extracting data from HTML and XML documents
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()

movie_reviews['review'] = movie_reviews['review'].apply(strip_html)

"""- **Remove punctuations**"""

# removing punctuations
def remove_punctuations(text):

    pattern = r'[^a-zA-Z0-9\s]'
    text = re.sub(pattern,'',text)

    # Single character removal
    text = re.sub(r"\s+[a-zA-Z]\s+", ' ', text)

    # Removing multiple spaces
    text = re.sub(r'\s+', ' ', text)

    return text

# Remove puntuations
movie_reviews['review'] = movie_reviews['review'].apply(remove_punctuations)

"""**Removing stopwords**

Stopwords are English words that do not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have etc.
"""

# setting english stopwords
stopword_list = nltk.corpus.stopwords.words('english')
print(stopword_list)

"""The above list of stopwords also contains the word "not", and its other forms such as don't, didn't, etc. We need them for correct sentiment classification.

For example, consider a negative review "*not a good movie*", and if we remove 'not' from it then it becomes a positive review "*a good movie*".
"""

# Exclude 'not' and its other forms from the stopwords list

updated_stopword_list = []

for word in stopword_list:
    if word=='not' or word.endswith("n't"):
        pass
    else:
        updated_stopword_list.append(word)

print(updated_stopword_list)

# removing the stopwords
def remove_stopwords(text, is_lower_case=False):
    # splitting strings into tokens (list of words)
    tokens = nltk.tokenize.word_tokenize(text)
    tokens = [token.strip() for token in tokens]
    if is_lower_case:
        # filtering out the stop words
        filtered_tokens = [token for token in tokens if token not in updated_stopword_list]
    else:
        filtered_tokens = [token for token in tokens if token.lower() not in updated_stopword_list]
    filtered_text = ' '.join(filtered_tokens)
    return filtered_text

# Apply function on review column
movie_reviews['review'] = movie_reviews['review'].apply(remove_stopwords)

movie_reviews.head()

# Convert sentiment labels to integers

movie_reviews['sentiment'] = movie_reviews['sentiment'].apply(lambda x: 1 if x=="positive" else 0)

movie_reviews.head()

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(movie_reviews['review'].values, movie_reviews['sentiment'].values,
                                                    test_size=0.20,
                                                    random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)

"""Prepare the Embedding Layer

The embedding layer converts our textual data into numeric data and is used as the first layer for the deep learning models in Keras
"""

# Tokenizer class from the keras.preprocessing.text module creates a word-to-index dictionary
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X_train)

X_train_tok = tokenizer.texts_to_sequences(X_train)
X_test_tok = tokenizer.texts_to_sequences(X_test)

"""The X_train set contains 40,000 lists of integers, each list corresponding to the sentences in a review. Set the maximum length of each list to 100 and add 0 padding to those lists that have a length $\lt$100, until they reach a length of 100."""

# Find the vocabulary size and perform padding on both train and test set
vocab_size = len(tokenizer.word_index) + 1

maxlen = 100

X_train_pad = pad_sequences(X_train_tok, padding='post', maxlen=maxlen, truncating='post')
X_test_pad = pad_sequences(X_test_tok, padding='post', maxlen=maxlen, truncating='post')

print ('number of unique words in the corpus:', vocab_size)

"""### Build LSTM Model for Text classification"""

EMBEDDING_DIM = 32

print('Build model...')

model = Sequential()
model.add(Embedding(input_dim = vocab_size, output_dim = EMBEDDING_DIM, input_length=maxlen))
model.add(LSTM(units=40,  dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

# Try using different optimizers and different optimizer configs
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

print('Summary of the built model...')
print(model.summary())

"""### Training and Validation"""

history = model.fit(X_train_pad, y_train, batch_size=128, epochs=3, verbose=1, validation_split=0.2)

"""### Model Performance"""

print('Testing...')
y_test = np.array(y_test)
score, acc = model.evaluate(X_test_pad, y_test, batch_size=128)

print('Test score:', score)
print('Test accuracy:', acc)

print("Accuracy: {0:.2%}".format(acc))

"""### Predict a few more samples"""

#Let us test some  samples
test_sample_1 = "This movie is fantastic! I really like it because it is so good!"
test_sample_2 = "Good movie!"
test_sample_3 = "Maybe I like this movie."
test_sample_4 = "Not to my taste, will skip and watch another movie"
test_sample_5 = "if you like action, then this movie might be good for you."
test_sample_6 = "Bad movie!"
test_sample_7 = "Not a good movie!"
test_sample_8 = "This movie really sucks! Can I get my money back please?"
test_samples = [test_sample_1, test_sample_2, test_sample_3, test_sample_4, test_sample_5, test_sample_6, test_sample_7, test_sample_8]

test_samples_tokens = tokenizer.texts_to_sequences(test_samples)
test_samples_tokens_pad = pad_sequences(test_samples_tokens, maxlen=maxlen)

# predict
pred = model.predict(x=test_samples_tokens_pad)
pred

"""The values close to 1 represents `positive` sentiment, and values close to 0 represents `negative` sentiment."""